# HW07 – Report

> Файл: `homeworks/HW07/report.md`  
> Важно: не меняйте названия разделов (заголовков). Заполняйте текстом и/или вставляйте результаты.

## 1. Datasets

Вы выбрали 3 датасета из 4 (перечислите): `S07-hw-dataset-01.csv`, `S07-hw-dataset-02.csv`, `S07-hw-dataset-03.csv`

### 1.1 Dataset A

- Файл: `S07-hw-dataset-01.csv`
- Размер: (12000, 9)
- Признаки: все числовые (8 признаков + sample_id)
- Пропуски: нет
- "Подлости" датасета: признаки в сильно разных шкалах (например, f02 и f07 имеют std ~60 и ~60, тогда как f01 и f03 — ~11 и ~0.5)

### 1.2 Dataset B

- Файл: `S07-hw-dataset-02.csv`
- Размер: (8000, 4)
- Признаки: все числовые (x1, x2, z_noise + sample_id)
- Пропуски: нет
- "Подлости" датасета: нелинейная структура + шумовой признак (z_noise) + возможные выбросы

### 1.3 Dataset C

- Файл: `S07-hw-dataset-03.csv`
- Размер: (15000, 5)
- Признаки: все числовые (x1, x2, f_corr, f_noise + sample_id)
- Пропуски: нет
- "Подлости" датасета: кластеры, вероятно, разной формы или с перекрытием; присутствует шумовой признак

## 2. Protocol

Опишите ваш "честный" unsupervised-протокол.

- Препроцессинг: применяли `StandardScaler` ко всем числовым признакам (обязательно, так как признаки в разных шкалах); пропусков и категориальных признаков не было — imputation и encoding не требовались; PCA применяли только для визуализации (2 компоненты), не для обучения.
- Поиск гиперпараметров:
  - KMeans: перебор `k` от 2 до 20, фиксировали `random_state=42`, `n_init=10`
  - DBSCAN: сетка `eps` ∈ [0.3, 0.5, 0.7, 1.0, 1.5, 2.0], `min_samples` ∈ [3, 5, 10]
  - лучшую модель выбирали по максимальному `silhouette_score`
- Метрики: `silhouette_score`, `davies_bouldin_score`, `calinski_harabasz_score`; для DBSCAN метрики считали только на non-noise точках, шум (`label=-1`) исключался из расчёта, но доля шума сохранялась отдельно.
- Визуализация: PCA(2D) для лучшего решения каждого датасета; дополнительно — графики `silhouette vs k` для KMeans.

## 3. Models

Перечислите, какие модели сравнивали **на каждом датасете**, и какие параметры подбирали.

Минимум (для каждого датасета):

- KMeans (поиск `k`, фиксировали `random_state=42`, `n_init=10`)
- DBSCAN (`eps`, `min_samples`, доля шума)

Опционально: третий метод не использовался.

## 4. Results

Для каждого датасета – краткая сводка результатов.

### 4.1 Dataset A

- Лучший метод и параметры: KMeans, k=2
- Метрики (silhouette / DB / CH): silhouette=0.522, DB=0.738, CH=2915.1
- Если был DBSCAN: не применимо
- Коротко: почему это решение выглядит разумным именно для этого датасета: данные компактные и сферические после масштабирования; идеальная устойчивость (ARI=1.0) подтверждает надёжность.

### 4.2 Dataset B

- Лучший метод и параметры: DBSCAN, eps=0.7, min_samples=3
- Метрики (silhouette / DB / CH): silhouette=0.346, DB=1.088, CH=2113.8
- Если был DBSCAN: доля шума = 0.7%
- Коротко: почему это решение выглядит разумным именно для этого датасета: нелинейная структура плохо подходит для KMeans; DBSCAN успешно выделяет плотные регионы и маркирует выбросы.

### 4.3 Dataset C

- Лучший метод и параметры: KMeans, k=3
- Метрики (silhouette / DB / CH): silhouette=0.316, DB=1.057, CH=4232.5
- Если был DBSCAN: не применимо
- Коротко: почему это решение выглядит разумным именно для этого датасета: кластеры компактные и одинаковой плотности; KMeans стабильно находит 3 группы, что соответствует ожидаемой структуре.

## 5. Analysis

### 5.1 Сравнение алгоритмов (важные наблюдения)

- KMeans "ломается" на Dataset B из-за нелинейной формы кластеров и шумового признака — он предполагает сферические кластеры одинаковой плотности.
- DBSCAN выигрывает именно там, где есть сложная геометрия или выбросы (Dataset B), но проигрывает при одинаковой плотности (Dataset A, C).
- Сильнее всего на результат повлияло **масштабирование**: без него KMeans "тянулся" к признакам с большей дисперсией, что полностью искажало кластеризацию.

### 5.2 Устойчивость (обязательно для одного датасета)

- Какую проверку устойчивости делали (5 запусков KMeans по разным seed или иной подход): 5 запусков KMeans с k=2 на Dataset A при разных `random_state` (10, 20, 30, 40, 50), оценка похожести через ARI.
- Что получилось (в 3-6 строк): все попарные ARI = 1.0, средний ARI = 1.0.
- Вывод: устойчиво/неустойчиво и почему вы так считаете: **высокоустойчиво**, потому что структура данных простая, кластеры хорошо разделены и компактны — KMeans всегда находит одно и то же разбиение.

### 5.3 Интерпретация кластеров

- Как вы интерпретировали кластеры: анализировали средние значения признаков в каждом кластере (профили).
- 3-6 строк выводов: в Dataset A кластеры различаются по всем признакам, особенно по f02 и f04; в Dataset B DBSCAN выделяет плотные скопления в пространстве (x1, x2), игнорируя z_noise; в Dataset C три группы явно различаются по x1 и x2. Шумовые признаки не доминируют в профилях — значит, кластеризация фокусируется на информативных переменных.

## 6. Conclusion

4-8 коротких тезисов: чему научились про кластеризацию, метрики и корректный протокол unsupervised-эксперимента.

- Без масштабирования distance-based методы бесполезны — это первое, что нужно проверить.
- KMeans хорош только для сферических кластеров одинаковой плотности; иначе — DBSCAN.
- Silhouette score — надёжный внутренний критерий, но требует ≥2 кластеров и осторожности с шумом.
- Честный unsupervised-эксперимент возможен: через фиксированный протокол, метрики и визуализацию.
- PCA(2D) — обязательный шаг для проверки "на глаз", даже если метрики высокие.
- Устойчивость — важна: хороший silhouette не гарантирует воспроизводимость.
- Нет "лучшего алгоритма" — только "лучший для данных".
- Даже без меток можно обоснованно выбирать модель.